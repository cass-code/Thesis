---
title: "Modelling Tobacco Demand: How the Illicit Cigarette Market Constrains the Legal Market"
subtitle: "Thesis Draft"

documentclass: "elsarticle"
AddTitle: TRUE # Use FALSE when submitting to peer reviewed platform. This will remove author names.
Author1: "Cassandra Pengelly"  # First Author - note the thanks message displayed as an italic footnote of first page.
Ref1: "Stellenbosch University" # First Author's Affiliation
CorrespAuthor_1: FALSE  # If corre
BottomRFooter: "\\footnotesize Page \\thepage" # Add a '#' before this line to 
margin: 2.3 # Sides
bottom: 2 # bottom
top: 2.5 # Top
HardSet_layout: TRUE 
linenumbers: FALSE # Used when submitting to journal
bibliography: Tex/ref.bib       # Do not edit: Keep this naming convention and location.
csl: Tex/harvard-stellenbosch-university.csl # referencing format used.
RemovePreprintSubmittedTo: TRUE 
toc: FALSE                       # Add a table of contents
numbersections: TRUE             # Should sections (and thus figures and tables) be numbered?
fontsize: 11pt                  # Set fontsize
linestretch: 1.2                # Set distance between lines.
link-citations: TRUE            # This creates dynamic links to the papers in reference list.
pagenumber: TRUE
output:
  pdf_document:
    keep_tex: TRUE
    template: Tex/TexDefault.txt
    fig_width: 3.5 # Adjust default figure sizes. This can also be done in the chunks of the text.
    fig_height: 3.5
# abstract: |
#   The illegal cigarette market has become an increasing problem in South Africa. 
---

# Introduction \label{Intro}

This thesis draft lays out the method and model used to study the relationship between the legal and illegal tobacco market in South Africa. Section \ref{dat} discusses the data used and how it was cleaned. Section \ref{Meth} explains the methodology, where a VECM and a VAR model are presented. The final section details discussion points (\ref{discuss}). The appendix \ref{app} contains the full model outputs. 

# Data \label{dat}

The sample period for this study runs from January 2012 to March 2020. Monthly data is used such that there are 99 observation points for each variable in the data set. One of the advantages of using monthly data rather than annual data is that it allows for more degrees of freedom. The data used includes figures for the prices and volumes of cigarettes in South Africa, tobacco excise duties, VAT, and disposable income. To prepare the data for analysis the most popular price category (MPPC) was identified as the 20-cigarette pack. Then a weighted average of before-tax 20-pack prices was used as a base price. The excise duty per 20's pack and VAT and were then added to the base price to calculate the price of licit cigarettes. The licit, illicit and disposable income amounts were adjusted for inflation, taking December 2016 as the base month and year. All of the variables have been transformed into log form. 

<!-- ![Time series plots](img/time_series_plots.png){width=100%, height=100%}\label{plot} -->

```{r, echo=FALSE, include=FALSE, warning=FALSE, results='hide', eval=TRUE}
#list.files('code/', full.names = T, recursive = T) %>% as.list() %>% walk(~source(.))

# read in the data - add the dataset's path to filePath
library(readxl)
filePath <- as.character("C:/Users/Cassandra/OneDrive/Documents/2021 Academics/Thesis/Thesis/data/Data.xlsx")
data <- read_excel(filePath)
source("code/data_clean.R")
log_data <- data_clean(data)
```

The figure below \ref{plot1} plots the time series of the logged variables. The graphs show that the data could be trending, which is formally tested in section \ref{Meth}.

```{r Figure2, echo=FALSE, warning =  FALSE, message=FALSE, fig.align = 'center', fig.cap = "Time Series Plot \\label{plot1}", fig.ext = 'png', fig.height = 5, fig.width = 6}
source("code/ts_plot_log.R")
ts1 <- ts_plot_log(log_data)
```

# Methodology \label{Meth}

To check whether the data is stationary, a number of tests is employed. First the autocorrelation functions are plotted below \ref{plot2}. They indicate that all four series are persistent; this is confirmed by the Ljung-Box tests in table \ref{box}. The Ljung-Box test for independence assesses whether there is significant evidence for non-zero correlations at a given lag, with the null hypothesis that there is independence in a given time series. A low p-value indicates a signal of non-stationarity. The augmented Dickey Fuller test (\ref {adf}) suggests that all four of the series contain a unit root (using the number of lags as 10^[Some of the series test as stationary when the number of lags is reduced]. This further suggests that the series are non-stationary. 

![\label{plot2} ACF Plots](img/ACF.png) 

```{r box, results = 'asis', echo=FALSE, warning =  FALSE, message=FALSE, fig.align = 'center', fig.cap = "Ljung \\label{box}", fig.ext = 'png', fig.height = 5, fig.width = 6}

library(xtable)
source("code/box_test.R")

box <- box_test(log_data)

data <- box %>% tibble::as_tibble()

table <- xtable(data, caption = "Ljung-Box Test \\label{box}")
  print.xtable(table,
             # tabular.environment = "longtable",
             floating = TRUE,
             table.placement = 'H',
             # scalebox = 0.3,
             comment = FALSE,
             caption.placement = 'bottom',
             include.rownames = FALSE
             )

```

```{r adf, results = 'asis', echo=FALSE, warning =  FALSE, message=FALSE, fig.align = 'center', fig.cap = "\\label{adf}", fig.ext = 'png', fig.height = 5, fig.width = 6}

library(xtable)
source("code/ADF.R")

adf <- ADF(log_data)

data <- adf %>% tibble::as_tibble()

table <- xtable(data, caption = "Augmented Dickey-Fuller Test \\label{adf}")
  print.xtable(table,
             # tabular.environment = "longtable",
             floating = TRUE,
             table.placement = 'H',
             # scalebox = 0.3,
             comment = FALSE,
             caption.placement = 'bottom',
             include.rownames = FALSE
             )

```

To assess whether a long-run relationship between the variables exists, the Johansen test is employed. According to Akaike's information criterion (AIC), the appropriate maximum number of lags to use is 10 (\ref{lag}). A lag order of 9 is used for the test, since the test requires a lag order of N - 1 = 10 - 1 = 9. Two Johansen tests are used: the Trace and the Maximum Eigenvalue tests. The Trace statistic test (\ref{coint}) shows we reject the null hypothesis that there are zero cointegrating relationships: the test statistic 85.85 is greater than the 1% significance level of 55.43. The test results indicate that there is 1 cointegrating relationship. Similarly, the Maximum Eigenvalue test rejects that there are zero cointegrating relationships, and fails to reject that there is at most 1 cointegrating relationship. The presence of a cointegrating vector amongst the variables suggests a Vector Error Correction Model is appropriate to analyse the variable dynamics.

```{r lag, results = 'asis', echo=FALSE, warning =  FALSE, message=FALSE, fig.align = 'center', fig.cap = "", fig.ext = 'png', fig.height = 5, fig.width = 6}

library(xtable)
source("code/lag.R")

lag <- lag(log_data, 10)

data <- lag %>% tibble::as_tibble()

table <- xtable(data, caption = "Optimal Lag Selection \\label{lag}")
  print.xtable(table,
             # tabular.environment = "longtable",
             floating = TRUE,
             table.placement = 'H',
             # scalebox = 0.3,
             comment = FALSE,
             caption.placement = 'bottom',
             include.rownames = FALSE
             )

```

```{r coint, results = 'asis', echo=FALSE, warning =  FALSE, message=FALSE, fig.align = 'center', fig.cap = "Cointegration Test Results\\label{coint}", fig.ext = 'png', fig.height = 5, fig.width = 6}

library(xtable)
library(tidyverse)
source("code/coint.R")
source("code/coint_trace.R")

ct <- coint_trace(log_data)
ce <- coint(log_data)


data <- ct #%>% tibble::as_tibble()

table <- xtable(data, caption = "Johansen Trace Test for Cointegration Results\\label{coint}")
  print.xtable(table,
             # tabular.environment = "longtable",
             floating = TRUE,
             table.placement = 'H',
             # scalebox = 0.3,
             comment = FALSE,
             caption.placement = 'bottom',
             include.rownames = TRUE
             )
  
data1 <- ce #%>% tibble::as_tibble()
  
table <- xtable(data1, caption = "Johansen Eigenvalue Test for Cointegration Results\\label{eigen}")
  print.xtable(table,
             # tabular.environment = "longtable",
             floating = TRUE,
             table.placement = 'H',
             # scalebox = 0.3,
             comment = FALSE,
             caption.placement = 'bottom',
             include.rownames = TRUE
             )

```

## Vector Error Correction Model 

A summary of the VECM results is given in \ref{vecm1}. The full model output with the 9 lags is given in the appendix \ref{app}. The results show that the error correction terms are not significant, with the exception of the cigarette quantity term, and very few of the first lag coefficients are significant.

```{r vecm, results = 'asis', echo=FALSE, warning =  FALSE, message=FALSE, fig.align = 'center', fig.cap = "Vector Error Correction Model 2OLS\\label{vecm1}", fig.ext = 'png', fig.height = 5, fig.width = 6}

source("code/VEC.R")
library(dplyr)
library(xtable)
vecresult <- VEC(log_data)
vecsum <- summary(vecresult) 

vecmatrix1 <- vecsum[[19]] %>% as.data.frame() %>% dplyr::select(1, 2) 

vecmatrix2 <- vecsum[[19]] %>% as.data.frame() %>% dplyr::select(3, 4, 5, 6) 
rownames(vecmatrix1) <- c("QDP", "PREALWAPDP", "PREALWAPDNP", "YDISPREAL")
rownames(vecmatrix2) <- c("QDP", "PREALWAPDP", "PREALWAPDNP", "YDISPREAL")
colnames(vecmatrix2) <- c("QDP-1", "PREALWAPDP-1", "PREALWAPDNP-1", "YDISPREAL-1")

# getting the results of the vecm; pulling out the coefficients and standard errors and forming them into a dataframe (only taking the first 6 columns)

table <- xtable(vecmatrix1, caption = "Vector Error Correction Model 2OLS\\label{vecm1}")
  print.xtable(table,
             tabular.environment = "longtable",
             floating = TRUE,
             table.placement = 'H',
             # scalebox = 0.3,
             comment = FALSE,
             caption.placement = 'bottom',
             include.rownames = TRUE
             )
  
table2 <- xtable(vecmatrix2, caption = "Vector Error Correction Model 2OLS\\label{vecm11}")
  print.xtable(table2,
             tabular.environment = "longtable",
             floating = TRUE,
             table.placement = 'H',
             # scalebox = 0.3,
             comment = FALSE,
             caption.placement = 'bottom',
             include.rownames = TRUE
             )

```

To test the accuracy of the model, a number of diagnostic tests were run. The majority of specification tests are modified for a Vector Autoregression model (VAR) so first the results of the VECM were converted to a VAR format. The Portmanteau serial correlation test shows that the null hypothesis of no autocorrelation is rejected at a 1% level of significance since the p-value is 0. The ARCH test shows that there are no ARCH effects (the test fails to reject the null). Finally, the normality test indicates the the residuals of the VECM model are not normally distributed. The serial correlation and normality test indicate that the model could be better specified. The next section explores an unstructured VAR model as an alternative.

```{r diagnostics, results = 'asis', echo=FALSE, warning =  FALSE, message=FALSE, fig.align = 'center', fig.cap = "Diagnostic Test Results\\label{diagnostic}", fig.ext = 'png', fig.height = 5, fig.width = 6}

library(xtable)
library(tidyverse)
source("code/diagnostic.R")
source("code/ARCH.R")
source("code/Norm.R")

serial <- diagnostic(log_data)
data <- serial %>% tibble::as_tibble()

table <- xtable(data, caption = "Serial Correlcation Test \\label{diag}")
  print.xtable(table,
             # tabular.environment = "longtable",
             floating = TRUE,
             table.placement = 'H',
             # scalebox = 0.3,
             comment = FALSE,
             caption.placement = 'bottom',
             include.rownames = FALSE
             )
  
arch <- ARCH(log_data)
data1 <- arch %>% tibble::as_tibble()
table1 <- xtable(data1, caption = "ARCH Tests \\label{archh}")
  print.xtable(table1,
             # tabular.environment = "longtable",
             floating = TRUE,
             table.placement = 'H',
             # scalebox = 0.3,
             comment = FALSE,
             caption.placement = 'bottom',
             include.rownames = FALSE
             )
 
norm <- Norm(log_data)
data2 <- norm %>% tibble::as_tibble()
table2 <- xtable(data2, caption = "Normality Tests \\label{archh}")
  print.xtable(table2,
             # tabular.environment = "longtable",
             floating = TRUE,
             table.placement = 'H',
             # scalebox = 0.3,
             comment = FALSE,
             caption.placement = 'bottom',
             include.rownames = FALSE
             )

```

## Unstructured VAR
If we proceed with using an unstructured VAR model despite the non-stationarity of the time series, we get the base model below (\ref{eq1}):  

\begin{align}
 Q_t = \mu + \sum_{i = 1}^{n}\beta_iQ_{t-i} +\sum_{i = 1}^{n}\gamma_iP_{t-i} + \sum_{i = 1}^{n}\theta_iY_{t-i} + \sum_{i = 1}^{n}\phi_iI_{t-i} \label{eq1}
\end{align}

where $Q_t$ is the log of cigarette consumption,\newline
$P_{t}$ is the log of real cigarette price,\newline
$Y_{t}$ is the log of real disposable income,\newline
$I_{t}$ is the log of real illicit cigarette price,\newline
$n$ is the number of lags and t is measured in months

The results of the VAR are presented in table \ref{varr} below. The diagnostic tests are summarised in the tables under the regression output in \ref{diagvar}. The VAR fairs better in the specification tests, although there is stil serial correlation present.

\newpage
```{r var, results = 'asis', echo=FALSE, warning =  FALSE, message=FALSE, fig.align = 'center', fig.cap = "Vector Error Correction Model 2OLS\\label{varr}", fig.ext = 'png', fig.height = 6, fig.width = 6}
source("code/VARS.R")
library(huxtable)
model <- VARS(log_data)

stargazer(model, header=FALSE, type = 'latex', single.row=TRUE, column.sep.width = "1pt", font.size= "footnotesize", no.space = TRUE, title = "Vector Autoregression \\label{varr}")
```

```{r diagnostics1, results = 'asis', echo=FALSE, warning =  FALSE, message=FALSE, fig.align = 'center', fig.cap = "Diagnostic Test Results\\label{diagnostic}", fig.ext = 'png', fig.height = 5, fig.width = 6}

library(xtable)
library(tidyverse)
source("code/diagvar.R")
source("code/ARCHvar.R")
source("code/Normvar.R")

serial <- diagvar(log_data)
data <- serial %>% tibble::as_tibble()

table <- xtable(data, caption = "Serial Correlcation Test \\label{diagvar}")
  print.xtable(table,
             # tabular.environment = "longtable",
             floating = TRUE,
             table.placement = 'H',
             # scalebox = 0.3,
             comment = FALSE,
             caption.placement = 'bottom',
             include.rownames = FALSE
             )
  
arch <- ARCHvar(log_data)
data1 <- arch %>% tibble::as_tibble()
table1 <- xtable(data1, caption = "ARCH Tests \\label{archh}")
  print.xtable(table1,
             # tabular.environment = "longtable",
             floating = TRUE,
             table.placement = 'H',
             # scalebox = 0.3,
             comment = FALSE,
             caption.placement = 'bottom',
             include.rownames = FALSE
             )
 
norm <- Normvar(log_data)
data2 <- norm %>% tibble::as_tibble()
table2 <- xtable(data2, caption = "Normality Tests \\label{archhvar}")
  print.xtable(table2,
             # tabular.environment = "longtable",
             floating = TRUE,
             table.placement = 'H',
             # scalebox = 0.3,
             comment = FALSE,
             caption.placement = 'bottom',
             include.rownames = FALSE
             )

```

The results of the VAR are then used to calculate elasticities according to the formula below:

\begin{equation}
\hat{E}_{d}=\frac{\sum_{t=1}^{9} \hat{\gamma}_{t}}{1-\sum_{t=1}^{9} \hat{\beta}_{t}}
\end{equation}

The elasticities are presented in the table below \ref{elastic}. The income elasticity is expected to be positive; however, it is unusual that both price elasticities are positive. 

```{r elasticity, results = 'asis', echo=FALSE, warning =  FALSE, message=FALSE, fig.align = 'center', fig.cap = "", fig.ext = 'png', fig.height = 5, fig.width = 6}

library(xtable)
library(tidyverse)
source("code/Quant.R")
source("code/PDP.R")
source("code/PDNP.R")
source("code/YDIS.R")

quantitydf<- Quant(log_data)
  quantsum <- sum(quantitydf$Coefficient) %>% as.numeric()
  
  quantsum1 <- 1 - (quantsum)
PDPdf<- PDP(log_data)
  pdpsum <- sum(PDPdf$Coefficient) %>% as.numeric()
PDNPdf<- PDNP(log_data)
  pdNpsum <- sum(PDNPdf$Coefficient) %>% as.numeric()
ydf<- YDIS(log_data)
  ysum <- sum(ydf$Coefficient) %>% as.numeric()

# Create table
a <- c("Illicit price elasticity of cigarette demand", "Income elasticity", "Price elasticity")
b <- c(pdNpsum/quantsum1, ysum/quantsum1, pdpsum/quantsum1)
dfinal <- data.frame(a,b)
elas <- dfinal %>% tibble::as_tibble()

table <- xtable(elas, caption = "Elasticities \\label{elastic}")
  print.xtable(table,
             # tabular.environment = "longtable",
             floating = TRUE,
             table.placement = 'H',
             # scalebox = 0.3,
             comment = FALSE,
             caption.placement = 'bottom',
             include.rownames = FALSE,
             include.colnames = FALSE
             )

```

# Discussion Points \label{discuss}

There are several concerns that I have regarding the data work:

* How do I choose the appropriate maximum number of lags for the stationarity tests? Changing the max lags impacted the optimal lag selection results, which then changed some results from nonstationary to stationary.

* If some of the time series are stationary and some are nonstationary, should I use a VECM or a VAR?

* How do I interperet the coefficients of the VAR (and VECM) for the elasticities? I think this is where I am going wrong in my final calculations.

\newpage

# Appendix \label{app} {-}

The full model output for the VECM is given below (for all 9 lags):
```{r vecmapp, results='asis', echo=FALSE, warning =  FALSE, message=FALSE, fig.align = 'center', fig.cap = "Vector Error Correction Model 2OLS\\label{vecmapp}", fig.ext = 'png', fig.height = 5, fig.width = 6}

source("code/VEC.R")
library(dplyr)
library(kableExtra)

vecresult <- VEC(log_data)
vecsum <- summary(vecresult) 
vecmatrix4 <- vecsum[[19]] %>% as.data.frame()
```

```{css, echo=FALSE}
.scroll-100 {
  max-height: 100px;
  overflow-y: auto;
  background-color: inherit;
}
```

```{r, class.output="scroll-100", echo=FALSE}
print(vecmatrix4)
```
<!-- # References {-} -->

<!-- <div id="refs"></div> -->






